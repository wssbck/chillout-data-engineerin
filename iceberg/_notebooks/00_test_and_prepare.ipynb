{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94146db3-4246-4e4c-be0f-6b71427a139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's test if boto3 can contact MinIO by creating the 'warehouse' S3 bucket\n",
    "\n",
    "import boto3\n",
    "\n",
    "s3_resource = boto3.resource('s3', \n",
    "    endpoint_url='http://minio:9000',\n",
    "    aws_access_key_id='minioadmin',\n",
    "    aws_secret_access_key='minioadmin',\n",
    "    aws_session_token=None,\n",
    "    config=boto3.session.Config(signature_version='s3v4'),\n",
    "    verify=False,\n",
    ")\n",
    "\n",
    "try:\n",
    "    s3_resource.Bucket(\"warehouse\").create()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360f35a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that Python client for Iceberg works\n",
    "\n",
    "from pyiceberg.catalog.rest import RestCatalog\n",
    "from pyiceberg.exceptions import NoSuchTableError\n",
    "\n",
    "\n",
    "catalog = RestCatalog(name=\"iceberg\", uri=\"http://nessie:19120/iceberg/main/\")\n",
    "\n",
    "try:\n",
    "    catalog.load_table(\"default.sample\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279eb712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that Spark client works\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .config(\n",
    "            \"spark.sql.extensions\",\n",
    "            \"org.projectnessie.spark.extensions.NessieSparkSessionExtensions, org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"\n",
    "        )\n",
    "        .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(\"spark.sql.catalog.iceberg.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\")\n",
    "        .config(\"spark.sql.catalog.iceberg.type\", \"rest\")\n",
    "        .config(\"spark.sql.catalog.iceberg.uri\", \"http://nessie:19120/iceberg/main/\")\n",
    "        .config(\"spark.sql.catalog.iceberg.ref\", \"main\")\n",
    "        .config(\"spark.sql.catalog.iceberg.cache-enabled\", False)\n",
    "        .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel('ERROR')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SHOW TABLES FROM iceberg.default\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that Trino client works\n",
    "\n",
    "from trino.dbapi import connect\n",
    "\n",
    "trino_connection = connect(\n",
    "    host=\"trino\",\n",
    "    port=8080,\n",
    "    user=\"trino\",\n",
    ")\n",
    "trino = trino_connection.cursor()\n",
    "trino.execute(\"SELECT * FROM system.runtime.nodes\")\n",
    "\n",
    "rows = trino.fetchall()\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c5f8bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# finally, let's check that avro tools work\n",
    "\n",
    "!java -jar /usr/bin/avro.jar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
